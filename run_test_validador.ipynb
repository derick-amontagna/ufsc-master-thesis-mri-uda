{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml.api import API\n",
    "from loguru import logger\n",
    "\n",
    "WORKSPACE_NAME = \"derick-amontagna\"\n",
    "\n",
    "\n",
    "def get_best_exp(project_name: str, metric_select: str = \"best_score\"):\n",
    "    api = API()\n",
    "    experiments = api.get(project_name=project_name, workspace=WORKSPACE_NAME)\n",
    "    best_experiment = None\n",
    "    best_dropout = None\n",
    "    best_g = None\n",
    "    best_score = 0\n",
    "\n",
    "    for exp in experiments:\n",
    "        metrics = exp.get_metrics()\n",
    "        name = exp.get_name()\n",
    "        for metric in metrics:\n",
    "            if metric[\"metricName\"] == metric_select:\n",
    "                score = float(metric[\"metricValue\"])\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_experiment = name\n",
    "\n",
    "    best_exp = api.get_experiment(WORKSPACE_NAME, project_name, best_experiment)\n",
    "    for parameters in best_exp.get_parameters_summary():\n",
    "        if parameters[\"name\"] == \"dropout\":\n",
    "            best_dropout = float(parameters[\"valueCurrent\"])\n",
    "    for parameters in best_exp.get_parameters_summary():\n",
    "        if parameters[\"name\"] == \"G_arch\":\n",
    "            best_g = parameters[\"valueCurrent\"]\n",
    "\n",
    "    logger.info(\"*\" * 25)\n",
    "    logger.info(\n",
    "        f\"Best exp_name: {best_experiment} || for best_score: {best_score} with dropout: {best_dropout} and {best_g}\"\n",
    "    )\n",
    "    return best_experiment, best_dropout, best_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml.api import API\n",
    "from loguru import logger\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "WORKSPACE_NAME = \"derick-amontagna\"\n",
    "\n",
    "\n",
    "def get_best_exp_mean(project_name: str):\n",
    "    api = API()\n",
    "    experiments = api.get(project_name=project_name, workspace=WORKSPACE_NAME)\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    for exp in experiments:\n",
    "        exp_data = {\n",
    "            \"experiment_id\": exp.id,\n",
    "            \"name\": exp.get_name(),\n",
    "            \"metrics\": exp.get_metrics(),\n",
    "            \"parameters\": exp.get_parameters_summary(),\n",
    "        }\n",
    "        data.append(exp_data)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    def extract_best_score(metrics_list):\n",
    "        best_scores = []\n",
    "        for metric in metrics_list:\n",
    "            if metric[\"metricName\"] == \"best_score\":\n",
    "                best_scores.append(metric[\"metricValue\"])\n",
    "        return max(best_scores)\n",
    "\n",
    "    def extract_parameters(parameters_list, name):\n",
    "        for parameter in parameters_list:\n",
    "            if parameter[\"name\"] == name:\n",
    "                output = parameter[\"valueCurrent\"]\n",
    "                break\n",
    "        return output\n",
    "\n",
    "    # Criar nova coluna somente com \"best_score\"\n",
    "    df[\"best_score\"] = df[\"metrics\"].apply(extract_best_score)\n",
    "\n",
    "    df[\"seed\"] = df[\"parameters\"].apply(lambda x: extract_parameters(x, \"seed\"))\n",
    "    df[\"lr\"] = df[\"parameters\"].apply(lambda x: extract_parameters(x, \"lr\"))\n",
    "    df[\"g_arch\"] = df[\"parameters\"].apply(lambda x: extract_parameters(x, \"G_arch\"))\n",
    "    df[\"best_score\"] = df[\"best_score\"].astype(float)\n",
    "\n",
    "    df.drop(columns=[\"metrics\", \"parameters\"], inplace=True)\n",
    "\n",
    "    df_mean = df.groupby([\"lr\", \"g_arch\"])[\"best_score\"].mean().reset_index()\n",
    "    df_mean.rename(columns={\"best_score\": \"mean_best_score\"}, inplace=True)\n",
    "\n",
    "    df_std = df.groupby([\"lr\", \"g_arch\"])[\"best_score\"].std().reset_index()\n",
    "    df_std.rename(columns={\"best_score\": \"std_best_score\"}, inplace=True)\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        pd.merge(df, df_mean, on=[\"lr\", \"g_arch\"], how=\"inner\"),\n",
    "        df_std,\n",
    "        on=[\"lr\", \"g_arch\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    return df_merged.sort_values(by=\"mean_best_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.pytorch_adapt.containers import Models\n",
    "from src.pytorch_adapt.models import Discriminator\n",
    "from common.networks import ARCHITECTURES, Classifier\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    G_arch=\"resnet50\", model_name=None, device=None, dropout=0.2, num_classes=2\n",
    "):\n",
    "    # Get the G\n",
    "    G = ARCHITECTURES[G_arch][\"model\"](weights=ARCHITECTURES[G_arch][\"weights\"])\n",
    "    if G_arch in [\"vgg16\", \"densenet161\", \"densenet201\"]:\n",
    "        G.classifier = nn.Identity()\n",
    "    else:\n",
    "        G.fc = nn.Identity()\n",
    "    feature_dim = {\n",
    "        \"resnet18\": 512,\n",
    "        \"resnet34\": 512,\n",
    "        \"resnet50\": 2048,\n",
    "        \"resnet101\": 2048,\n",
    "        \"vgg16\": 25088,\n",
    "    }[G_arch]\n",
    "\n",
    "    G_state_dict = torch.load(\n",
    "        os.path.join(\n",
    "            \"checkpoints\",\n",
    "            f\"{model_name}.pth\",\n",
    "        )\n",
    "    )[\n",
    "        \"models\"\n",
    "    ][\"G\"]\n",
    "    G.load_state_dict(G_state_dict, strict=True)\n",
    "    G = G.to(device)\n",
    "    hidden_size = {\n",
    "        \"resnet18\": 256,\n",
    "        \"resnet34\": 256,\n",
    "        \"resnet50\": 512,\n",
    "        \"resnet101\": 2048,\n",
    "    }[G_arch]\n",
    "\n",
    "    # Get the C\n",
    "    C = Classifier(\n",
    "        in_size=feature_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout=dropout,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    C_state_dict = torch.load(\n",
    "        os.path.join(\n",
    "            \"checkpoints\",\n",
    "            f\"{model_name}.pth\",\n",
    "        )\n",
    "    )[\n",
    "        \"models\"\n",
    "    ][\"C\"]\n",
    "    C.load_state_dict(C_state_dict, strict=True)\n",
    "    C = C.to(device)\n",
    "    # Get the D\n",
    "    D = Discriminator(in_size=feature_dim)\n",
    "    D = D.to(device)\n",
    "    return Models({\"G\": G, \"C\": C, \"D\": D})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from src.pytorch_adapt.utils.common_functions import batch_to_device\n",
    "from src.pytorch_adapt.validators import AccuracyValidator, AUCValidator\n",
    "\n",
    "\n",
    "def gen_data_score(\n",
    "    model: torch.nn.Module,\n",
    "    dataloaders: torch.utils.data.DataLoader,\n",
    "    data_type: str = \"target_val_with_labels\",\n",
    "    device: torch.device = None,\n",
    "    exp=None,\n",
    "):\n",
    "    logger.info(f\"Eval - {data_type}\")\n",
    "    model.eval()\n",
    "    G, C = model[\"G\"], model[\"C\"]\n",
    "    labels, logits, preds = [], [], []\n",
    "    data_side = data_type.split(\"_\")[0]\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloaders[data_type]):\n",
    "            data = batch_to_device(data, device)\n",
    "            logit = C(G(data[f\"{data_side}_imgs\"]))\n",
    "            if isinstance(logit, list):\n",
    "                logit = logit[0]\n",
    "            pred = F.softmax(logit, dim=-1)\n",
    "            logits.append(logit)\n",
    "            preds.append(pred)\n",
    "            if f\"{data_side}_labels\" in data:\n",
    "                label = data[f\"{data_side}_labels\"]\n",
    "                labels.append(label)\n",
    "        logits = torch.cat(logits, dim=0)\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        if labels:\n",
    "            labels = torch.cat(labels, dim=0)\n",
    "            data_score = {\"logits\": logits, \"preds\": preds, \"labels\": labels}\n",
    "        else:\n",
    "            data_score = {\"logits\": logits, \"preds\": preds}\n",
    "    return data_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from loguru import logger\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_adapt.datasets import (\n",
    "    DataloaderCreator,\n",
    "    SourceDataset,\n",
    "    TargetDataset,\n",
    "    CombinedSourceAndTargetDataset,\n",
    "    ConcatDataset,\n",
    ")\n",
    "\n",
    "from common.CustomData.MRI_NII_2D import Dataset2D\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "\n",
    "class GrayscaleToRGB:\n",
    "    def __call__(self, x):\n",
    "        if x.size(0) == 3:\n",
    "            return x\n",
    "        elif x.size(0) == 1:\n",
    "            return torch.cat([x, x, x], dim=0)\n",
    "        else:\n",
    "            raise Exception(\"Image is not grayscale (or even RGB).\")\n",
    "\n",
    "\n",
    "class RandomRotFlip:\n",
    "    def __call__(self, image):\n",
    "        k = torch.randint(\n",
    "            0, 4, (1,)\n",
    "        ).item()  # Rotação aleatória (0, 90, 180, 270 graus)\n",
    "        image = torch.rot90(image, k, dims=(-2, -1))\n",
    "\n",
    "        if torch.rand(1).item() > 0.5:\n",
    "            image = torch.flip(image, dims=[-1])  # Flip horizontal\n",
    "        if torch.rand(1).item() > 0.5:\n",
    "            image = torch.flip(image, dims=[-2])  # Flip vertical\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class RandomRotate:\n",
    "    def __init__(self, angle_range=(-20, 20)):\n",
    "        self.angle_range = angle_range\n",
    "\n",
    "    def __call__(self, image):\n",
    "        angle = random.randint(\n",
    "            self.angle_range[0], self.angle_range[1]\n",
    "        )  # Ângulo aleatório\n",
    "        return transforms.functional.rotate(\n",
    "            image, angle, interpolation=transforms.InterpolationMode.NEAREST\n",
    "        )\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            GrayscaleToRGB(),\n",
    "            RandomRotFlip(),\n",
    "            RandomRotate(angle_range=(-20, 20)),\n",
    "            transforms.ToDtype(torch.float32),\n",
    "            # MinMaxNormalize(),\n",
    "        ]\n",
    "    ),\n",
    "    \"val_test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            GrayscaleToRGB(),\n",
    "            transforms.ToDtype(torch.float32),\n",
    "            # MinMaxNormalize(),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "DOMAINS = {\n",
    "    \"ADNI1-GE\": os.path.join(\n",
    "        \"data\", \"ADNI1-T1-AD-CN\", \"Image\", \"Preprocess\", \"6_step_nifti_2d\", \"GE\"\n",
    "    ),\n",
    "    \"ADNI1-Philips\": os.path.join(\n",
    "        \"data\", \"ADNI1-T1-AD-CN\", \"Image\", \"Preprocess\", \"6_step_nifti_2d\", \"Philips\"\n",
    "    ),\n",
    "    \"ADNI1-Siemens\": os.path.join(\n",
    "        \"data\", \"ADNI1-T1-AD-CN\", \"Image\", \"Preprocess\", \"6_step_nifti_2d\", \"Siemens\"\n",
    "    ),\n",
    "    \"ADNI1-GE-3D\": os.path.join(\n",
    "        \"data\", \"ADNI1-T1-AD-CN\", \"Image\", \"Preprocess\", \"5_step_class_folders\", \"GE\"\n",
    "    ),\n",
    "    \"ADNI1-Philips-3D\": os.path.join(\n",
    "        \"data\",\n",
    "        \"ADNI1-T1-AD-CN\",\n",
    "        \"Image\",\n",
    "        \"Preprocess\",\n",
    "        \"5_step_class_folders\",\n",
    "        \"Philips\",\n",
    "    ),\n",
    "    \"ADNI1-Siemens-3D\": os.path.join(\n",
    "        \"data\",\n",
    "        \"ADNI1-T1-AD-CN\",\n",
    "        \"Image\",\n",
    "        \"Preprocess\",\n",
    "        \"5_step_class_folders\",\n",
    "        \"Siemens\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def create_dataloaders_mri_2d(\n",
    "    source: str,\n",
    "    target: str,\n",
    "    transform_train: transforms.Compose = data_transforms[\"train\"],\n",
    "    transform_val_test: transforms.Compose = data_transforms[\"val_test\"],\n",
    "    algorithm: str = \"source-only\",\n",
    "    validator: str = \"Accuracy\",\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 0,\n",
    "    seed: int = None,\n",
    "):\n",
    "    logger.info(f\"Loading Source and Target Datasets\".center(70, \"+\"))\n",
    "    data_output = {\n",
    "        \"src\": {\"train\": {}, \"val_test\": {}},\n",
    "        \"target\": {\"train\": {}, \"val_test\": {}},\n",
    "    }\n",
    "    for domain_side, domain in zip([\"src\", \"target\"], [source, target]):\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            data = Dataset2D(\n",
    "                domain=DOMAINS[domain], split=split, transform=transform_val_test\n",
    "            )\n",
    "            data_output[domain_side][\"val_test\"][split] = data\n",
    "            if split in [\"train\", \"val\"] and domain_side == \"src\":\n",
    "                data_transform = Dataset2D(\n",
    "                    domain=DOMAINS[domain], split=split, transform=transform_train\n",
    "                )\n",
    "                data_output[domain_side][\"train\"][split] = data_transform\n",
    "            elif split == \"train\" and domain_side == \"target\":\n",
    "                data_transform = Dataset2D(\n",
    "                    domain=DOMAINS[domain], split=split, transform=transform_train\n",
    "                )\n",
    "                data_output[domain_side][\"train\"][split] = data_transform\n",
    "\n",
    "    logger.info(f\"Create Source and Target Datasets\".center(70, \"+\"))\n",
    "    dataset = {}\n",
    "    if algorithm == \"source-only\":\n",
    "        dataset[\"src_train\"] = SourceDataset(data_output[\"src\"][\"train\"][\"train\"])\n",
    "    else:\n",
    "        dataset[\"src_train\"] = SourceDataset(data_output[\"src\"][\"val_test\"][\"train\"])\n",
    "    dataset[\"src_val\"] = SourceDataset(data_output[\"src\"][\"val_test\"][\"val\"])\n",
    "    dataset[\"src_test\"] = SourceDataset(data_output[\"src\"][\"val_test\"][\"test\"])\n",
    "\n",
    "    dataset[\"target_train\"] = TargetDataset(data_output[\"target\"][\"val_test\"][\"train\"])\n",
    "    dataset[\"target_val\"] = TargetDataset(data_output[\"target\"][\"val_test\"][\"val\"])\n",
    "    dataset[\"target_test\"] = TargetDataset(data_output[\"target\"][\"val_test\"][\"test\"])\n",
    "\n",
    "    dataset[\"target_train_with_labels\"] = TargetDataset(\n",
    "        data_output[\"target\"][\"val_test\"][\"train\"], domain=1, supervised=True\n",
    "    )\n",
    "    dataset[\"target_val_with_labels\"] = TargetDataset(\n",
    "        data_output[\"target\"][\"val_test\"][\"val\"], domain=1, supervised=True\n",
    "    )\n",
    "    dataset[\"target_test_with_labels\"] = TargetDataset(\n",
    "        data_output[\"target\"][\"val_test\"][\"test\"], domain=1, supervised=True\n",
    "    )\n",
    "\n",
    "    dataset[\"train\"] = CombinedSourceAndTargetDataset(\n",
    "        SourceDataset(data_output[\"src\"][\"train\"][\"train\"]),\n",
    "        TargetDataset(data_output[\"target\"][\"train\"][\"train\"]),\n",
    "    )\n",
    "\n",
    "    if algorithm == \"source-only\":\n",
    "        train_names, val_names = [\"src_train\"], [\n",
    "            \"train\",\n",
    "            \"src_val\",\n",
    "            \"src_test\",\n",
    "            \"target_train\",\n",
    "            \"target_train_with_labels\",\n",
    "            \"target_val\",\n",
    "            \"target_val_with_labels\",\n",
    "            \"target_test\",\n",
    "            \"target_test_with_labels\",\n",
    "        ]\n",
    "    else:\n",
    "        train_names, val_names = [\"train\"], [\n",
    "            \"src_train\",\n",
    "            \"src_val\",\n",
    "            \"src_test\",\n",
    "            \"target_train\",\n",
    "            \"target_train_with_labels\",\n",
    "            \"target_val\",\n",
    "            \"target_val_with_labels\",\n",
    "            \"target_test\",\n",
    "            \"target_test_with_labels\",\n",
    "        ]\n",
    "\n",
    "    logger.info(f\"Create Dataloader\".center(70, \"+\"))\n",
    "    dc = DataloaderCreator(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        train_names=train_names,\n",
    "        val_names=val_names,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    dataloaders = dc(**dataset)\n",
    "    target_dataset_size = len(dataset[\"target_train\"])\n",
    "    logger.info(f\"Finishing the Creation of Dataloaders\".center(70, \"+\"))\n",
    "    return dataloaders, target_dataset_size, train_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp\n",
    "# name, seed, g = get_best_exp(project_name=\"adni1-so-siemens\")\n",
    "source = \"ADNI1-GE\"\n",
    "target = \"ADNI1-Siemens\"\n",
    "project_name = \"adni1-so-ge-real\"\n",
    "\n",
    "# Init process\n",
    "df = get_best_exp_mean(project_name=project_name)\n",
    "df_3 = df.head(3).sort_values(by=\"best_score\", ascending=False)\n",
    "name = df_3[\"name\"].iloc[0]\n",
    "g = df_3[\"g_arch\"].iloc[0]\n",
    "seed = df_3[\"seed\"].iloc[0]\n",
    "\n",
    "\n",
    "results = []\n",
    "for i in range(3):\n",
    "    result = {}\n",
    "    name = df_3[\"name\"].iloc[i]\n",
    "    g = df_3[\"g_arch\"].iloc[i]\n",
    "    seed = df_3[\"seed\"].iloc[i]\n",
    "    result['name'] = name\n",
    "    # Model\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(\n",
    "        G_arch=g, model_name=name, dropout=0.5, device=device\n",
    "    ) \n",
    "\n",
    "    # Data\n",
    "    # from common.data_setup import create_dataloaders_mri_2d\n",
    "    dataloaders, target_dataset_size, train_name = create_dataloaders_mri_2d(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        algorithm=\"source-only\",\n",
    "        batch_size=128,\n",
    "        num_workers=8,\n",
    "        seed=int(seed),\n",
    "    )\n",
    "\n",
    "    # Gen Scores\n",
    "    data_score_source = gen_data_score(model, dataloaders, \"src_\" + \"test\", device, None)\n",
    "    data_score_target = gen_data_score(\n",
    "        model, dataloaders, \"target_\" + \"train\" + \"_with_labels\", device, None\n",
    "    )\n",
    "    result['Acc_Source'] = AccuracyValidator()(**{\"src_val\": data_score_source})\n",
    "    result['Acc_Target'] = AccuracyValidator()(**{\"src_val\": data_score_target})\n",
    "    result['AUC_Source'] = AUCValidator()(**{\"src_val\": data_score_source})\n",
    "    result['AUC_Target'] = AUCValidator()(**{\"src_val\": data_score_target})\n",
    "    results.append(result)\n",
    "\n",
    "#'ultimate_fixture_9510' - GE\n",
    "# 'loud_shrimp_6199' - Philips\n",
    "# 'regional_purlin_4276' - Siemens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "# from common.data_setup import create_dataloaders_mri_2d\n",
    "source = \"ADNI1-GE\"\n",
    "target = \"ADNI1-Siemens\"\n",
    "dataloaders, target_dataset_size, train_name = create_dataloaders_mri_2d(\n",
    "    source=source,\n",
    "    target=target,\n",
    "    algorithm=\"source-only\",\n",
    "    batch_size=128,\n",
    "    num_workers=8,\n",
    "    seed=int(seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen Scores\n",
    "data_score_source = gen_data_score(model, dataloaders, \"src_\" + \"test\", device, None)\n",
    "data_score_target = gen_data_score(\n",
    "    model, dataloaders, \"target_\" + \"train\" + \"_with_labels\", device, None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AccuracyValidator()(**{\"src_val\": data_score_source})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AccuracyValidator()(**{\"src_val\": data_score_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCValidator()(**{\"src_val\": data_score_source})  # 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCValidator()(**{\"src_val\": data_score_target})  # 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
